#!/usr/bin/python

# ------------------------------------------------------------------------------
# File: eosarchiverd
# Author: Elvin-Alin Sindrilaru <esindril@cern.ch>
# ------------------------------------------------------------------------------
#
# ******************************************************************************
# EOS - the CERN Disk Storage System
# Copyright (C) 2014 CERN/Switzerland
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
# ******************************************************************************

import os
import json
import zmq
import const
import logging
import glob
import stat
import exceptions
import subprocess
import daemon
from logging import handlers
from multiprocessing import Process
from sys import exit, getsizeof
from os.path import join
from hashlib import sha256
from archivefile import ArchiveFile, NoErrorException
from errno import EIO, EINVAL
from veryprettytable import VeryPrettyTable


# Constants
const.BATCH_SIZE = 5      # max number of transfers to be performed in parallel
const.POLLTIMEOUT = 1000  # miliseconds
const.CREATE_OP = 'create'
const.GET_OP = 'get'
const.PUT_OP = 'put'
const.LIST_OP = 'list'
const.PURGE_OP = 'purge'
const.DELETE_OP = 'delete'
const.OPT_RETRY = 'retry'
const.ARCH_FN = ".archive"
const.LOG_DIR = "/var/log/eos/archive/"
const.DIR = {const.GET_OP: "/tmp/eos_archive_get/",
             const.PUT_OP: "/tmp/eos_archive_put/",
             const.PURGE_OP: "/tmp/eos_archive_purge/",
             const.DELETE_OP: "/tmp/eos_archive_delete/"}
const.MAX_PENDING = 10  # max number of requests allowed in pending
const.IPC_FILE = "/tmp/archivebackend.ipc"
const.LOG_FORMAT = ('%(asctime)-15s %(name)s[%(process)d] %(filename)s:'
                    '%(lineno)d:LVL=%(levelname)s %(message)s')

# TODO: maybe parse the xrd.cf.mgm to get the configuration info


def do_transfer(eos_file, op, opt):
  """ Execute a transfer job.

  Args:
    eos_file (string): EOS location of the archive file
    op       (string): operation type: get/put
    opt      (string): option for the transfer: recover/purge
  """
  arch = ArchiveFile(eosf=eos_file, operation=op, option=opt)

  try:
    arch.run()
  except IOError as e:
    logger = logging.getLogger(__name__)
    arch.logger.error(e)
    arch.clean_transfer(False)
    exit(EIO)
  except NoErrorException as e:
    arch.clean_transfer(True)
  '''
  except Exception as e:
    logger = logging.getLogger("Dispatcher")
    arch.logger.error(e)
    arch.clean_transfer(False)
    exit(EINVAL)
  '''


class Dispatcher(object):
  """ Dispatcher daemon responsible for receiving requests from the clients
  and then spawning the proper executing process to get, put or purge.

  Attributes:
    proc (dict): Dictionary containing the currently running processes for
      both put and get.
    pending (dict): Dictionary containing the currently pending requests for
      both put and get.
    max_proc (int): Max number of concurrent proceeses of one type allowed.
  """
  def __init__(self):
    self.logger = logging.getLogger(type(self).__name__)
    log_file = const.LOG_DIR + "eosarchiver.log"
    formatter = logging.Formatter(const.LOG_FORMAT)
    rotate_handler = logging.handlers.TimedRotatingFileHandler(log_file, 'midnight')
    rotate_handler.setFormatter(formatter)
    self.logger.addHandler(rotate_handler)
    self.logger.propagate = False
    self.proc = self.pending = self.orphan = {}

    for op_type in [const.GET_OP, const.PUT_OP, const.PURGE_OP, const.DELETE_OP]:
      self.proc[op_type] = {}
      self.pending[op_type] = {}
      self.orphan[op_type] = {}

  def run(self):
    """ Server entry point which is responsible for spawning worker proceesses
    that do the actual transfers (put/get).
    """
    ctx = zmq.Context()
    self.logger.debug("Started dispatcher process")
    socket = ctx.socket(zmq.REP)
    socket.bind("ipc://" + const.IPC_FILE)

    try:
      os.chmod(const.IPC_FILE, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
    except OSError as e:
      self.logger.error("Could not set permissions on IPC socket file:{0}".
                        format(const.IPC_FILE))
      raise

    poller = zmq.Poller()
    poller.register(socket, zmq.POLLIN)

    # Attach orphaned processes and put them in a special list so that
    # one can monitor them in case the tracker is restarted
    self.get_orphans()

    while True:
      events = dict(poller.poll(const.POLLTIMEOUT))

      # Update worker processes status
      for op, dict_jobs in self.proc.iteritems():
        remove_elem = []
        for uuid, proc in dict_jobs.iteritems():
          if not proc.is_alive():
            proc.join()
            self.logger.debug("Job={0}, pid={1}, exitcode={2}".
                              format(uuid, proc.pid, proc.exitcode))
            remove_elem.append(uuid)

        for uuid in remove_elem:
          try:
            del self.proc[op][uuid]
          except ValueError as e:
            self.logger.error("Unable to remove job={0} from list".format(uuid))

        del remove_elem[:]

      # Update orphan worker processes status
      for op, dict_jobs in self.orphan.iteritems():
        remove_elem = []
        for uuid, info_pair in dict_jobs.iteritems():
          try:
            os.kill(int(info_pair[0]), 0)
          except Exception as e:
            self.logger.debug("Job={0}, pid={1}, path={2} finshed, error={3}".
                              format(uuid, info_pair[0], info_pair[1], e))
            remove_elem.append(uuid)

        for uuid in remove_elem:
          try:
            del self.orphan[op][uuid]
          except ValueError as e:
            self.logger.error("Unable to remove job={0} from orphan list".format(uuid))

        del remove_elem[:]

      # Submit any pending jobs
      self.submit_pending()

      if events and events.get(socket) == zmq.POLLIN:
        try:
          req_json = socket.recv_json()
        except zmq.ZMQError as e:
          if e.errno == zmq.ETERM:
            break  # shutting down, exit
          else:
            raise
        except ValueError as e:
          self.logger.error("Command in not in JSON format")
          socket.send("ERROR error:command not in JSON format")
          continue

        self.logger.debug("Received command: {0}".format(req_json))
        op = req_json['cmd']

        if op in [const.PUT_OP, const.GET_OP, const.PURGE_OP, const.DELETE_OP]:
          src, opt = req_json['src'], req_json['opt']
          root_src = src[:-(len(src) - src.rfind('/') - 1)]
          job_uuid = sha256(root_src).hexdigest()
          self.logger.info("Add job: {0}".format(job_uuid))

          # TODO: check also in orphans list
          if job_uuid in self.proc[op]:
            self.logger.error("Transfer with same signature already exists")
            socket.send("ERROR error: transfer with same signature exists")
            continue

          if job_uuid not in self.pending[op]:
            if len(self.pending[op]) <= const.MAX_PENDING:
              self.pending[op][job_uuid] = (src, op, opt)
              self.submit_pending()
              socket.send("OK id=" + job_uuid)
            else:
              socket.send("ERROR too many pending requests, resubmit later")
          else:
            socket.send("ERROR error:transfer already queued")

        elif op == const.LIST_OP:
          reply = self.do_list(req_json)
          socket.send_string(reply)
        else:
          # Operation not supported reply to client with error
          self.logger.debug("ERROR operation not supported: {0}".format(msg))
          socket.send("ERROR error:operation not supported")

  def submit_pending(self):
    """ Submit as many pending requests as possible if there are enough available
    workers to process them.
    """
    for op, dict_jobs in self.pending.iteritems():
      if dict_jobs:
        remove_elem = []
        num_proc = len([_ for _, proc in self.proc[op] if proc.is_alive()])
        self.logger.debug("Num. running processes: {0}".format(num_proc))

        for job_uuid, req_tuple in dict_jobs.iteritems():
          if num_proc < const.BATCH_SIZE:
            self.logger.info("Pending request is submitted ...")
            proc = Process(target=do_transfer, args=(req_tuple))
            self.proc[op][job_uuid] = proc
            proc.start()
            remove_elem.append(job_uuid)
            num_proc += 1
          else:
            self.logger.debug("No more workers available")
            break

        for key in remove_elem:
          del self.pending[op][key]

        del remove_elem[:]

  def get_orphans(self):
    """ Get the running proceesses so that we can monitor their evolution. We do
        this by listing all the status files in /tmp/eos_archive_*/*.ps
    """
    list_ps = glob.glob("/tmp/eos_archive_*/*.ps")

    if list_ps:
      for ps_file in list_ps:
        self.logger.debug("ps file: {0}".format(ps_file))
        for key, val in const.DIR.iteritems():
          if val in ps_file:
            # Get the uuid of the job
            uuid = ps_file[ps_file.rfind('/') + 1: ps_file.rfind('.')]
            # Get the pid which is running the job
            ps_proc = subprocess.Popen(['tail', '-1', ps_file],
                                       stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            ps_out, ps_err = ps_proc.communicate()
            ps_out = ps_out.strip('\0')
            pid = ps_out[ps_out.find("pid=") + 4: ps_out.find(' ')]
            # Check if process is still alive
            try:
              os.kill(int(pid), 0)
            except Exception as e:
              self.logger.info("Process {0} is no longer alive, error:{1}".format(pid, e))
              tx_file = ps_file[:ps_file.find('.')] + '.tx'
              log_file = ps_file[:ps_file.find('.')] + '.log'
              # Delete all files associated to this transfer
              try:
                os.remove(ps_file)
              except OSError as e1:
                pass

              try:
                os.remove(tx_file)
              except OSError as e1:
                pass

              try:
                os.remove(log_file)
              except OSError as e1:
                pass
            else:
              # Read the path from the transfer file
              tx_file = ps_file[: ps_file.rfind('.')] + '.tx'
              with open(tx_file, 'r') as f:
                header = json.loads(f.readline())
                path = header['src']
                self.orphan[key][uuid] = (pid, path)
                self.logger.debug("op={0}, uuid={1}, pid={2}".format(key, uuid, pid))

  def do_list(self, req_json):
    """ List the transfers.

    Args:
      req_json (JSON command): Listing command in JSON format.

    Returns:
      String with the result of the listing to be returned to the client.
    """
    msg = "OK "
    ls_type = req_json['opt']
    self.logger.debug("Listing type: {0}".format(ls_type))
    table = VeryPrettyTable()
    table.field_names = ["Id", "Path", "Type", "State", "Message"]

    if ls_type in [const.PUT_OP, const.GET_OP, const.PURGE_OP, const.DELETE_OP]:

      for uuid in self.orphan[ls_type]:
        pid, path = self.orphan[ls_type][uuid]
        ps_file = "".join([const.DIR[ls_type], uuid, ".ps"])
        ps_proc = subprocess.Popen(['tail', '-1', ps_file],
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        ps_out, ps_err = ps_proc.communicate()
        ps_out = ps_out.strip('\0')
        ps_msg = ps_out[ps_out.find("msg=") + 4:]
        table.add_row([uuid, path, ls_type, "running (o)", ps_msg])

      for uuid in self.proc[ls_type]:
        path = self.proc[ls_type][uuid]._args[0]
        path = path[path.rfind("//") + 1: path.rfind("/")]
        ps_file = "".join([const.DIR[ls_type], uuid, ".ps"])
        ps_proc = subprocess.Popen(['tail', '-1', ps_file],
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        ps_out, ps_err = ps_proc.communicate()
        ps_out = ps_out.strip('\0')
        ps_msg = ps_out[ps_out.find("msg=") + 4:]
        table.add_row([uuid, path, ls_type, "running", ps_msg])

      for uuid in self.pending[ls_type]:
        path = self.pending[ls_type][uuid][0]
        path = path[path.rfind("//") + 1: path.rfind("/")]
        table.add_row([uuid, path, ls_type, "pending", "none"])

    elif ls_type == "all":
      for ls_type in self.proc:
        for uuid in self.proc[ls_type]:
          path = self.proc[ls_type][uuid]._args[0]
          path = path[path.rfind("//") + 1: path.rfind("/")]
          ps_file = "".join([const.DIR[ls_type], uuid, ".ps"])
          ps_proc = subprocess.Popen(['tail', '-1', ps_file],
                                     stdout=subprocess.PIPE,
                                     stderr=subprocess.PIPE)
          ps_out, ps_err = ps_proc.communicate()
          ps_out = ps_out.strip('\0')
          ps_msg = ps_out[ps_out.find("msg=") + 4:]
          table.add_row([uuid, path, ls_type, "running", ps_msg])

        for uuid in self.orphan[ls_type]:
          pid, path = self.orphan[ls_type][uuid]
          ps_file = "".join([const.DIR[ls_type], uuid, ".ps"])
          ps_proc = subprocess.Popen(['tail', '-1', ps_file],
                                     stdout=subprocess.PIPE,
                                     stderr=subprocess.PIPE)
          ps_out, ps_err = ps_proc.communicate()
          ps_out = ps_out.strip('\0')
          ps_msg = ps_out[ps_out.find("msg=") + 4:]
          table.add_row([uuid, path, ls_type, "running (o)", ps_msg])

        for uuid in self.pending[ls_type]:
          path = self.pending[ls_type][uuid][0]
          path = path[path.rfind("//") + 1: path.rfind("/")]
          table.add_row([uuid, path, ls_type, "pending", "none"])
    else:
      # TODO ls_typ can be a job_uuid then only return the status
      # of the requested job
      msg = "ERROR error:unsupported operation: {0}".format(ls_type)

    msg += table.get_string()
    return msg


def main():
  logging.basicConfig(level=logging.DEBUG, format=const.LOG_FORMAT)

  # Create the local directory structure
  for op in [const.PUT_OP, const.GET_OP, const.PURGE_OP, const.DELETE_OP]:
    try:
      os.mkdir(const.DIR[op])
    except OSError as e:
      pass

  dispatcher = Dispatcher()
  dispatcher.run()

with daemon.DaemonContext():
  main()

# if __name__ == '__main__':
#   main()
